# Data
With the goal of this project being to investigate the correlation between sports betting rates and education/income trends across U.S. states, we collected and compiled data from multiple sources. The first dataset, titled Online Sports Betting Dataset, was the most straightforward. This data included information about the 31 legalized states in sports betting and was retrieved from <https://rg.org/statistics/us>. Since the data was not contained in an easily scraped format, we downloaded it manually by switching between the year toggle button to get access to all the publicly available data. In doing this, we were able to gain sports betting data from 26 states as multiple states have sports bettting activity through tribal casinos or have only been legalized in the year 2025 so there is not data available yet. Since the process of getting this data was straightforward, it did require any additional cleaning.

In contrast, collecting income and education data from the U.S. Census (covering the years 2018â€“2023) proved more complex. The range from 2018-2023 was selected because it represents the overlap of available years across our datasets. However, due to the structure of the Census API, each year and state combination was available at a different API endpoint. Additionally, the topic parameter (either income or education) had to be specified separately. This meant that retrieving data for the 31 included states across six years, for two topics, would have required 384 manual downloads and subsequent joins. 

To avoid this, we built a web scraper using Docker, Supabase, PostgreSQL, and Beekeeper Studio. From Census, we were able to grab a variable identifier called `GeoID` which are numeric codes that uniquely identify all geographical areas for which the Census Bureau. In our case, this contains all the ids that identify the states we want to scrape for.

The scraper looped through the `year` and `GeoID` values in the API URL to pull each required combination in one run. However, we opted to run the scraper twice, once for income and once for education, manually replacing the topic ID in the `scraper.py` file between runs, rather than building the scraper to run both topics. The logic behind this decision was that it was simpler to change one number in the `scraper.py` file and run it a second time, than create a scraper that switched which table it was dumping into halfway through, since storing education and income in two individual tables was better for the planned analysis. 

To automate this process and manage dependencies across our local devices, we used Docker. Docker allowed us to contain our scraping environment so that all required libraries, API keys, and files could be packaged into a single reproducible environment. This ensured that the scraper would run identically regardless of the local setup and all us to deploy the scraper in a controlled environment,

For data storage, we used Supabase which is an open-source backend service platform that provides a hosted PostgreSQL database along with API keys and file storage features. This acted as a cloud layer that hosted our PostgreSQL instance, making it accessible to both members of our team without having to manage database infrastructure ourselves. Supabase connects directly from our local tools for easy querying and inspection. We then stored all the scraped data in Beekeeper to clean and work with.

Once scraped, we employed SQL to clean and normalize the raw JSON responses into a table format. Rather than storing all columns from the original census datasets, we selected only the variables relevant to our analysis. This process resulted in three final tables: the sports betting dataset, the cleaned income dataset, and the cleaned education dataset, all of which were stored on local systems for the remainder of the project. For each dataset, we performed several operations to ensure consistency across columns and formats. We first conducted basic data cleaning and wrangling where the column values and names were altered to be consistent among all the data. The next step was finding a common variable among our three datasets. `Geoid` and `year` became our primary identifiers across our sports betting, cleaned income, and cleaned education datasets. This also allowed for simple screening for duplicates among the tables that may have the same geoid and year, and we were able to filter those out quickly. We created a `location` table that showed the corresponding state name to each `geoID`, which worked as a connection between all tables. After that we normalized the data structure in a way we believed was efficient for our planned analysis.

As seen Figure 1, to support analysis of the relationship between sports betting activity and socioeconomic trends across U.S. states, the data was structured into a schema that separates income, education, and betting metrics into connected tables to facilitate flexible joins. `Geoid` and `year` serve as foreign keys linking all domain-specific tables (income, education, and sports betting) to the central location table that maps each geoid to a state-level `place_name`. Income data is organized into two fact tables. The `income_distribution` table captures the percentage of households within each income bracket and household type, while the `income_summary` table provides metrics such as total households, median income, and mean income by household type. These reference the `income_brackets` and `household_type` dimension tables for consistency. Education data is stored in the `edu_long` table, which reports the number of individuals by education level and age group. It joins the `age_group_map` table to normalize age categories. Sports betting data is stored in `clean_sports`, which contains metrics such as handle, revenue, hold, and taxes. This table integrates with the rest of the schema via shared identifiers. This structure allows for efficient querying as we explore the research question.

<figure id="fig-erd">
  <img src="figures/figure1a.png" alt="ERD diagram" width="800">
  <figcaption style="font-size: 8px; color: #555;">
    Figure 1: This figure displays an ERD normalized to third normal form (3NF), highlighting primary keys and table relationships. All our tables are in some way connected to the location table.
  </figcaption>
</figure>
